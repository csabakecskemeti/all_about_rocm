{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c01fbe-e283-4f9f-ac12-41aa34523cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rocm.blogs.amd.com/artificial-intelligence/bnb-8bit/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac57396-6262-472a-a829-31374e934058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38cb0b-0b4e-40e7-9aac-cf173e445aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5ForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "import numpy as np\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from sys import argv\n",
    "\n",
    "dataset = load_dataset(\"glue\", data_dir=\"cola\")\n",
    "metric = load(\"glue\", \"cola\")\n",
    "model_checkpoint = \"google-t5/t5-3b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def preprocess_fun(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True)\n",
    "\n",
    "dataset = dataset.map(preprocess_fun, batched=True)\n",
    "\n",
    "model = T5ForSequenceClassification.from_pretrained(model_checkpoint, device_map='cuda:0', torch_dtype=torch.float16)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    f\"T5-finetuned-cola\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # print(predictions)\n",
    "    predictions = np.argmax(predictions[0], axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "if argv[-1]=='1':\n",
    "    print(\"Using bnb's 8-bit Adam optimizer\")\n",
    "    adam = bnb.optim.Adam8bit(model.parameters())\n",
    "else:\n",
    "    adam = None # defaults to regular Adam\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=dataset[\"validation\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers = (adam,None)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ad972-6ce7-4424-9b59-369f83d0e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=quantization_config, \n",
    "    torch_dtype=torch.float16 # Loads model in float16 and quantizes to INT8 and moves to GPU device\n",
    ")\n",
    "print(model_8bit.device)\n",
    "inp = torch.randint(100,(1,20))\n",
    "inp = inp.to(\"cuda:0\")\n",
    "print(model_8bit.generate(inp, max_new_tokens=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852ffbf9-b8ad-4b58-82a8-5ed33cd00ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 14.2.0-4ubuntu2) 14.2.0\n",
      "Copyright (C) 2024 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6658d024004476b8c1c405afe668d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Yer lookin' fer a chat with a trusty pirate, eh? Alright then, matey! Me name be Captain Chatty, the scurviest chatbot on the seven seas! I be here to help ye with yer questions, share me knowledge, and swab the decks o' confusion! Just hoist the sails and set course fer adventure, me hearty!\"}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    # model_kwargs={\"torch_dtype\": torch.bfloat16, \"load_in_8bit\": True},\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"load_in_4bit\": True},\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d0ef9-f064-4016-b213-db425a3f36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!TORCH_BLAS_PREFER_HIPBLASLT=0\n",
    "!export TORCH_BLAS_PREFER_HIPBLASLT\n",
    "!TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb9f62c-5f3d-4004-9a43-4d6b9d8740d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ROCBLAS_USE_HIPBLASLT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002c4a89-e291-474d-a3b8-be6ad713597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 14.2.0-4ubuntu2) 14.2.0\n",
      "Copyright (C) 2024 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005ef82e1a85441f8732439d1e9b971b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke. Share a joke with me. I'll share one\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig  \n",
    "device = \"cuda:0\"\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=False)\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "\tmodel_name, device_map=device, torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_text = \"Tell me a joke.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "output = quantized_model.generate(**input_ids, max_new_tokens=10)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a748bb9-bfa9-4f5c-8a4e-b86b812f9232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
